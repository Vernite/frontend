import*as arrays from"../../../base/common/arrays.js";import{onUnexpectedError}from"../../../base/common/errors.js";import{LineTokens}from"../tokens/lineTokens.js";import{TokenizationRegistry}from"../languages.js";import{nullTokenizeEncoded}from"../languages/nullTokenize.js";import{Disposable}from"../../../base/common/lifecycle.js";import{StopWatch}from"../../../base/common/stopwatch.js";import{countEOL}from"../core/eolCounter.js";import{ContiguousMultilineTokensBuilder}from"../tokens/contiguousMultilineTokensBuilder.js";import{runWhenIdle}from"../../../base/common/async.js";import{setTimeout0}from"../../../base/common/platform.js";class ContiguousGrowingArray{constructor(_default){this._default=_default,this._store=[]}get(index){return index<this._store.length?this._store[index]:this._default}set(index,value){for(;index>=this._store.length;)this._store[this._store.length]=this._default;this._store[index]=value}delete(deleteIndex,deleteCount){0===deleteCount||deleteIndex>=this._store.length||this._store.splice(deleteIndex,deleteCount)}insert(insertIndex,insertCount){if(0===insertCount||insertIndex>=this._store.length)return;const arr=[];for(let i=0;i<insertCount;i++)arr[i]=this._default;this._store=arrays.arrayInsert(this._store,insertIndex,arr)}}export class TokenizationStateStore{constructor(tokenizationSupport,initialState){this.tokenizationSupport=tokenizationSupport,this.initialState=initialState,this._lineBeginState=new ContiguousGrowingArray(null),this._lineNeedsTokenization=new ContiguousGrowingArray(!0),this._firstLineNeedsTokenization=0,this._lineBeginState.set(0,this.initialState)}get invalidLineStartIndex(){return this._firstLineNeedsTokenization}markMustBeTokenized(lineIndex){this._lineNeedsTokenization.set(lineIndex,!0),this._firstLineNeedsTokenization=Math.min(this._firstLineNeedsTokenization,lineIndex)}getBeginState(lineIndex){return this._lineBeginState.get(lineIndex)}setEndState(linesLength,lineIndex,endState){if(this._lineNeedsTokenization.set(lineIndex,!1),this._firstLineNeedsTokenization=lineIndex+1,lineIndex===linesLength-1)return;const previousEndState=this._lineBeginState.get(lineIndex+1);if(null===previousEndState||!endState.equals(previousEndState))return this._lineBeginState.set(lineIndex+1,endState),void this.markMustBeTokenized(lineIndex+1);let i=lineIndex+1;for(;i<linesLength&&!this._lineNeedsTokenization.get(i);)i++;this._firstLineNeedsTokenization=i}applyEdits(range,eolCount){this.markMustBeTokenized(range.startLineNumber-1),this._lineBeginState.delete(range.startLineNumber,range.endLineNumber-range.startLineNumber),this._lineNeedsTokenization.delete(range.startLineNumber,range.endLineNumber-range.startLineNumber),this._lineBeginState.insert(range.startLineNumber,eolCount),this._lineNeedsTokenization.insert(range.startLineNumber,eolCount)}}export class TextModelTokenization extends Disposable{constructor(_textModel,_tokenizationPart,_languageIdCodec){super(),this._textModel=_textModel,this._tokenizationPart=_tokenizationPart,this._languageIdCodec=_languageIdCodec,this._isScheduled=!1,this._isDisposed=!1,this._tokenizationStateStore=null,this._register(TokenizationRegistry.onDidChange((e=>{const languageId=this._textModel.getLanguageId();-1!==e.changedLanguages.indexOf(languageId)&&(this._resetTokenizationState(),this._tokenizationPart.clearTokens())}))),this._resetTokenizationState()}dispose(){this._isDisposed=!0,super.dispose()}handleDidChangeContent(e){if(e.isFlush)this._resetTokenizationState();else{if(this._tokenizationStateStore)for(let i=0,len=e.changes.length;i<len;i++){const change=e.changes[i],[eolCount]=countEOL(change.text);this._tokenizationStateStore.applyEdits(change.range,eolCount)}this._beginBackgroundTokenization()}}handleDidChangeAttached(){this._beginBackgroundTokenization()}handleDidChangeLanguage(e){this._resetTokenizationState(),this._tokenizationPart.clearTokens()}_resetTokenizationState(){const[tokenizationSupport,initialState]=initializeTokenization(this._textModel,this._tokenizationPart);this._tokenizationStateStore=tokenizationSupport&&initialState?new TokenizationStateStore(tokenizationSupport,initialState):null,this._beginBackgroundTokenization()}_beginBackgroundTokenization(){!this._isScheduled&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._isScheduled=!0,runWhenIdle((deadline=>{this._isScheduled=!1,this._backgroundTokenizeWithDeadline(deadline)})))}_backgroundTokenizeWithDeadline(deadline){const endTime=Date.now()+deadline.timeRemaining(),execute=()=>{!this._isDisposed&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._backgroundTokenizeForAtLeast1ms(),Date.now()<endTime?setTimeout0(execute):this._beginBackgroundTokenization())};execute()}_backgroundTokenizeForAtLeast1ms(){const lineCount=this._textModel.getLineCount(),builder=new ContiguousMultilineTokensBuilder,sw=StopWatch.create(!1);do{if(sw.elapsed()>1)break;if(this._tokenizeOneInvalidLine(builder)>=lineCount)break}while(this._hasLinesToTokenize());this._tokenizationPart.setTokens(builder.finalize(),this._isTokenizationComplete())}tokenizeViewport(startLineNumber,endLineNumber){const builder=new ContiguousMultilineTokensBuilder;this._tokenizeViewport(builder,startLineNumber,endLineNumber),this._tokenizationPart.setTokens(builder.finalize(),this._isTokenizationComplete())}reset(){this._resetTokenizationState(),this._tokenizationPart.clearTokens()}forceTokenization(lineNumber){const builder=new ContiguousMultilineTokensBuilder;this._updateTokensUntilLine(builder,lineNumber),this._tokenizationPart.setTokens(builder.finalize(),this._isTokenizationComplete())}getTokenTypeIfInsertingCharacter(position,character){if(!this._tokenizationStateStore)return 0;this.forceTokenization(position.lineNumber);const lineStartState=this._tokenizationStateStore.getBeginState(position.lineNumber-1);if(!lineStartState)return 0;const languageId=this._textModel.getLanguageId(),lineContent=this._textModel.getLineContent(position.lineNumber),text=lineContent.substring(0,position.column-1)+character+lineContent.substring(position.column-1),r=safeTokenize(this._languageIdCodec,languageId,this._tokenizationStateStore.tokenizationSupport,text,!0,lineStartState),lineTokens=new LineTokens(r.tokens,text,this._languageIdCodec);if(0===lineTokens.getCount())return 0;const tokenIndex=lineTokens.findTokenIndexAtOffset(position.column-1);return lineTokens.getStandardTokenType(tokenIndex)}tokenizeLineWithEdit(position,length,newText){const lineNumber=position.lineNumber,column=position.column;if(!this._tokenizationStateStore)return null;this.forceTokenization(lineNumber);const lineStartState=this._tokenizationStateStore.getBeginState(lineNumber-1);if(!lineStartState)return null;const curLineContent=this._textModel.getLineContent(lineNumber),newLineContent=curLineContent.substring(0,column-1)+newText+curLineContent.substring(column-1+length),languageId=this._textModel.getLanguageIdAtPosition(lineNumber,0),result=safeTokenize(this._languageIdCodec,languageId,this._tokenizationStateStore.tokenizationSupport,newLineContent,!0,lineStartState);return new LineTokens(result.tokens,newLineContent,this._languageIdCodec)}isCheapToTokenize(lineNumber){if(!this._tokenizationStateStore)return!0;const firstInvalidLineNumber=this._tokenizationStateStore.invalidLineStartIndex+1;return!(lineNumber>firstInvalidLineNumber)&&(lineNumber<firstInvalidLineNumber||this._textModel.getLineLength(lineNumber)<2048)}_hasLinesToTokenize(){return!!this._tokenizationStateStore&&this._tokenizationStateStore.invalidLineStartIndex<this._textModel.getLineCount()}_isTokenizationComplete(){return!!this._tokenizationStateStore&&this._tokenizationStateStore.invalidLineStartIndex>=this._textModel.getLineCount()}_tokenizeOneInvalidLine(builder){if(!this._tokenizationStateStore||!this._hasLinesToTokenize())return this._textModel.getLineCount()+1;const lineNumber=this._tokenizationStateStore.invalidLineStartIndex+1;return this._updateTokensUntilLine(builder,lineNumber),lineNumber}_updateTokensUntilLine(builder,lineNumber){if(!this._tokenizationStateStore)return;const languageId=this._textModel.getLanguageId(),linesLength=this._textModel.getLineCount(),endLineIndex=lineNumber-1;for(let lineIndex=this._tokenizationStateStore.invalidLineStartIndex;lineIndex<=endLineIndex;lineIndex++){const text=this._textModel.getLineContent(lineIndex+1),lineStartState=this._tokenizationStateStore.getBeginState(lineIndex),r=safeTokenize(this._languageIdCodec,languageId,this._tokenizationStateStore.tokenizationSupport,text,!0,lineStartState);builder.add(lineIndex+1,r.tokens),this._tokenizationStateStore.setEndState(linesLength,lineIndex,r.endState),lineIndex=this._tokenizationStateStore.invalidLineStartIndex-1}}_tokenizeViewport(builder,startLineNumber,endLineNumber){if(!this._tokenizationStateStore)return;if(endLineNumber<=this._tokenizationStateStore.invalidLineStartIndex)return;if(startLineNumber<=this._tokenizationStateStore.invalidLineStartIndex)return void this._updateTokensUntilLine(builder,endLineNumber);let nonWhitespaceColumn=this._textModel.getLineFirstNonWhitespaceColumn(startLineNumber);const fakeLines=[];let initialState=null;for(let i=startLineNumber-1;nonWhitespaceColumn>1&&i>=1;i--){const newNonWhitespaceIndex=this._textModel.getLineFirstNonWhitespaceColumn(i);if(0!==newNonWhitespaceIndex&&(newNonWhitespaceIndex<nonWhitespaceColumn&&(fakeLines.push(this._textModel.getLineContent(i)),nonWhitespaceColumn=newNonWhitespaceIndex,initialState=this._tokenizationStateStore.getBeginState(i-1),initialState)))break}initialState||(initialState=this._tokenizationStateStore.initialState);const languageId=this._textModel.getLanguageId();let state=initialState;for(let i=fakeLines.length-1;i>=0;i--){state=safeTokenize(this._languageIdCodec,languageId,this._tokenizationStateStore.tokenizationSupport,fakeLines[i],!1,state).endState}for(let lineNumber=startLineNumber;lineNumber<=endLineNumber;lineNumber++){const text=this._textModel.getLineContent(lineNumber),r=safeTokenize(this._languageIdCodec,languageId,this._tokenizationStateStore.tokenizationSupport,text,!0,state);builder.add(lineNumber,r.tokens),this._tokenizationStateStore.markMustBeTokenized(lineNumber-1),state=r.endState}}}function initializeTokenization(textModel,tokenizationPart){if(textModel.isTooLargeForTokenization())return[null,null];const tokenizationSupport=TokenizationRegistry.get(tokenizationPart.getLanguageId());if(!tokenizationSupport)return[null,null];let initialState;try{initialState=tokenizationSupport.getInitialState()}catch(e){return onUnexpectedError(e),[null,null]}return[tokenizationSupport,initialState]}function safeTokenize(languageIdCodec,languageId,tokenizationSupport,text,hasEOL,state){let r=null;if(tokenizationSupport)try{r=tokenizationSupport.tokenizeEncoded(text,hasEOL,state.clone())}catch(e){onUnexpectedError(e)}return r||(r=nullTokenizeEncoded(languageIdCodec.encodeLanguageId(languageId),state)),LineTokens.convertToEndOffset(r.tokens,text.length),r}